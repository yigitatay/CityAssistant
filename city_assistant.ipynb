{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import textstat\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "questions_df = pd.read_csv('./data/resident_request_questions.csv')\n",
    "context_df = pd.read_csv('./data/dc_service_requests.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model\n",
    "    )\n",
    "    return np.array(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_type</th>\n",
       "      <th>department</th>\n",
       "      <th>resolution_estimate</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abandoned Bicycle</td>\n",
       "      <td>DPW, DPW</td>\n",
       "      <td>20 bd</td>\n",
       "      <td>This service request is to be used for bicycle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abandoned Vehicle - On Private Property</td>\n",
       "      <td>DPW, DPW</td>\n",
       "      <td>45 bd</td>\n",
       "      <td>Please use this service request to request the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abandoned Vehicle - On Public Property</td>\n",
       "      <td>DPW, DPW</td>\n",
       "      <td>13 bd</td>\n",
       "      <td>Please use this service request to request the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alley Repair Investigation</td>\n",
       "      <td>DDOT</td>\n",
       "      <td>270 bd</td>\n",
       "      <td>Please use this service request type to invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bee Treatment and Inspection (DOH)</td>\n",
       "      <td>DOH</td>\n",
       "      <td>14 bd</td>\n",
       "      <td>Bee Treatment - This service request is limite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Tree Inspection</td>\n",
       "      <td>DDOT</td>\n",
       "      <td>5 bd</td>\n",
       "      <td>Use this request type to report an urgent tree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Tree Planting</td>\n",
       "      <td>DDOT</td>\n",
       "      <td>500 bd</td>\n",
       "      <td>Urban Forestry Administration (UFA) plants nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Tree Pruning</td>\n",
       "      <td>DDOT</td>\n",
       "      <td>180 bd</td>\n",
       "      <td>Please use this service type to request a publ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Tree Removal</td>\n",
       "      <td>DDOT</td>\n",
       "      <td>180 bd</td>\n",
       "      <td>Please use this service type to request the re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Vacant Lot - Public Property Only</td>\n",
       "      <td>DPW</td>\n",
       "      <td>50 bd</td>\n",
       "      <td>Please use this service request type to reques...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               request_type department resolution_estimate  \\\n",
       "0                         Abandoned Bicycle   DPW, DPW               20 bd   \n",
       "1   Abandoned Vehicle - On Private Property   DPW, DPW               45 bd   \n",
       "2    Abandoned Vehicle - On Public Property   DPW, DPW               13 bd   \n",
       "3                Alley Repair Investigation       DDOT              270 bd   \n",
       "4        Bee Treatment and Inspection (DOH)        DOH               14 bd   \n",
       "..                                      ...        ...                 ...   \n",
       "82                          Tree Inspection       DDOT                5 bd   \n",
       "83                            Tree Planting       DDOT              500 bd   \n",
       "84                             Tree Pruning       DDOT              180 bd   \n",
       "85                             Tree Removal       DDOT              180 bd   \n",
       "86        Vacant Lot - Public Property Only        DPW               50 bd   \n",
       "\n",
       "                                          Description  \n",
       "0   This service request is to be used for bicycle...  \n",
       "1   Please use this service request to request the...  \n",
       "2   Please use this service request to request the...  \n",
       "3   Please use this service request type to invest...  \n",
       "4   Bee Treatment - This service request is limite...  \n",
       "..                                                ...  \n",
       "82  Use this request type to report an urgent tree...  \n",
       "83  Urban Forestry Administration (UFA) plants nea...  \n",
       "84  Please use this service type to request a publ...  \n",
       "85  Please use this service type to request the re...  \n",
       "86  Please use this service request type to reques...  \n",
       "\n",
       "[87 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(context_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_estimate_helper(res_estimate):\n",
    "    resolution_estimate = res_estimate.split(' ')[0]\n",
    "    bd_or_cd = res_estimate.split(' ')[1]\n",
    "    resolution_estimate += ' business days' if bd_or_cd == 'bd' else ' calendar days'\n",
    "    return resolution_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Type: Abandoned Bicycle\n",
      "Department: DPW, DPW\n",
      "Resolution Estimate: 20 business days\n",
      "Description: This service request is to be used for bicycles that are left on public space and which are abandoned.\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "\n",
    "for idx, row in context_df.iterrows():\n",
    "    combined_text = (\n",
    "        f\"Request Type: {row['request_type']}\\n\"\n",
    "        f\"Department: {row['department']}\\n\"\n",
    "        f\"Resolution Estimate: {res_estimate_helper(row['resolution_estimate'])}\\n\"\n",
    "        f\"Description: {row['Description']}\"\n",
    "    )\n",
    "    \n",
    "    embed_vec = get_embedding(combined_text)\n",
    "    all_embeddings.append(embed_vec)\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = all_embeddings.astype(np.float32)\n",
    "embedding_dim = all_embeddings.shape[1]\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "faiss_index.add(all_embeddings)\n",
    "faiss.write_index(faiss_index, \"dc_requests.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To simply load pre-calculated, run this:\n",
    "faiss_index = faiss.read_index(\"dc_requests.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dc_requests(query: str, top_k: int = 3):\n",
    "    query_vec = get_embedding(query).astype(np.float32).reshape(1, -1)\n",
    "    # Search FAISS index\n",
    "    distances, indices = faiss_index.search(query_vec, top_k)\n",
    "\n",
    "    results = []\n",
    "    for rank, idx in enumerate(indices[0]):\n",
    "        row_data = context_df.iloc[idx]\n",
    "        dist = distances[0][rank]\n",
    "        results.append({\n",
    "            \"request_type\": row_data[\"request_type\"],\n",
    "            \"department\": row_data[\"department\"],\n",
    "            \"resolution_estimate\": res_estimate_helper(row_data[\"resolution_estimate\"]),\n",
    "            \"description\": row_data[\"Description\"],\n",
    "            \"distance\": float(dist),\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'request_type': 'Pothole', 'department': 'DDOT', 'resolution_estimate': '3 business days', 'description': 'Please use this request type for Pothole investigation. Pothole repairs normally take approximately 3 business days (72 hours), weather permitting, for completion.\\n\\n', 'distance': 0.23399491608142853}, {'request_type': 'Roadway Repair', 'department': 'DDOT', 'resolution_estimate': '270 business days', 'description': 'Please use this service request type to investigate street surface issues. Please provide the specific location (i.e. address, intersection) and describe the specific repair problem (i.e. uneven pavement, numerous potholes). Also if possible, provide any information regarding the street surfaces paving material (i.e. concrete, asphalt, or brick).\\n\\n', 'distance': 0.3189466595649719}, {'request_type': 'Alley Repair Investigation', 'department': 'DDOT', 'resolution_estimate': '270 business days', 'description': 'Please use this service request type to investigate alley surfaces issues. Please identify the specific alley location, provide a description of problem (i.e. pothole, re-paving) and describe the type of existing alley surface (i.e. dirt, concrete, brick). It is also helpful to provide any additional information specific to the proposed repair.', 'distance': 0.32842686772346497}]\n"
     ]
    }
   ],
   "source": [
    "query = \"How long does it take to fix a pothole?\"\n",
    "dc_matches = search_dc_requests(query, top_k=3)\n",
    "print(dc_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_query):\n",
    "    # 1. Retrieve relevant requests from context\n",
    "    dc_results = search_dc_requests(user_query, top_k=3)\n",
    "    \n",
    "    # 2. Create context string\n",
    "    context_lines = []\n",
    "    for res in dc_results:\n",
    "        context_lines.append(\n",
    "            f\"Request Type: {res['request_type']}\\n\"\n",
    "            f\"Department: {res['department']}\\n\"\n",
    "            f\"Resolution Estimate: {res['resolution_estimate']}\\n\"\n",
    "            f\"Description: {res['description']}\\n\"\n",
    "            f\"Distance: {res['distance']}\\n\"\n",
    "            \"----\"\n",
    "        )\n",
    "    context_str = \"\\n\".join(context_lines)\n",
    "    \n",
    "    # 3. Build the final prompt\n",
    "    SYSTEM_PROMPT = \"You are a QA system that assists with resident inquiries and service requests in Washington D.C.\"\n",
    "    FINAL_PROMPT = (\n",
    "        f\"{SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"Context from Washington D.C. service requests:\\n{context_str}\\n\\n\"\n",
    "        f\"User's question: {user_query}\\n\"\n",
    "        f\"Provide clear, concise, and legally compliant responses.\"\n",
    "        f\"Make sure your response is easily readable and understandable by a layman.\" \n",
    "        f\"If the answer doesn't belong to one of the request types, state that you're not sure.\"\n",
    "        f\"Answer format:\"\n",
    "        f\"- The content of your answer\"\n",
    "        f\"- Used request type: The request type you used\"\n",
    "    )\n",
    "    \n",
    "    # 4. Call API\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": FINAL_PROMPT},\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_completeness(ai_response, required_fields):\n",
    "    \"\"\"\n",
    "    A naive completeness check that ensures the AI response mentions \n",
    "    certain keywords or phrases. 'required_fields' can be a list of items \n",
    "    we expect to see in the answer (e.g., department, resolution, etc.).\n",
    "\n",
    "    Returns True if all required fields appear, otherwise False.\n",
    "    \"\"\"\n",
    "    response_lower = ai_response.lower()\n",
    "    for field in required_fields:\n",
    "        if field.lower() not in response_lower:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprompt_for_correctness(query, ai_response, context_info):\n",
    "    \"\"\"\n",
    "    Calls the LLM again to check whether the AI's answer is correct \n",
    "    given the context_info (e.g., the row from dc_service_requests).\n",
    "    Returns a dict with \"is_correct\" and \"revised_answer\" or similar fields.\n",
    "    \n",
    "    For demonstration, we do a ChatCompletion call that we parse.\n",
    "    In production, you might want more robust JSON parsing or error handling.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"You are a QA system verifying correctness of the AI’s response.\"\n",
    "    user_prompt = f\"\"\"\n",
    "User Query: {query}\n",
    "\n",
    "AI Response:\n",
    "{ai_response.split('Used request type:')[0]}\n",
    "\n",
    "Relevant Context (from official data):\n",
    "{context_info}\n",
    "\n",
    "Task:\n",
    "1. Check if the AI's response is factually correct and consistent with the context.\n",
    "2. If incorrect or incomplete, propose a corrected version.\n",
    "3. Return your findings in the following JSON format:\n",
    "{{\n",
    "  \"is_correct\": true or false,\n",
    "  \"revised_answer\": \"text\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Attempt to parse a JSON-like structure from the content\n",
    "    # We'll do a simple regex to find a JSON block, then use Python's `json` if well-formed\n",
    "    try:\n",
    "        # find a JSON substring\n",
    "        json_match = re.search(r\"\\{.*\\}\", content, flags=re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "            parsed = json.loads(json_str)\n",
    "            return parsed\n",
    "        else:\n",
    "            return {\n",
    "                \"is_correct\": False,\n",
    "                \"revised_answer\": \"Could not parse JSON from LLM response\"\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"is_correct\": False,\n",
    "            \"revised_answer\": f\"Error parsing LLM output: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_with_rules(query, ai_response, context_df, request_type):\n",
    "    \"\"\"\n",
    "    Checks if the AI response obeys known rules from context_df and overall readability guidelines.\n",
    "    Returns a dictionary with flags, metrics, and/or suggested corrections.\n",
    "    \"\"\"\n",
    "    evaluation_result = {\n",
    "        \"flesch_reading_ease\": None, \n",
    "        \"gunning_fog\": None,\n",
    "        \"potential_request_types\": None,\n",
    "        \"rt_complete\": True,\n",
    "        \"resolution_estimate_complete\": True,\n",
    "        \"complete\": True\n",
    "    }\n",
    "\n",
    "    # 1. Check readability\n",
    "    evaluation_result['flesch_reading_ease'] = textstat.flesch_reading_ease(ai_response)\n",
    "    evaluation_result['gunning_fog'] = textstat.gunning_fog(ai_response)\n",
    "    \n",
    "    # 2. Check if request type is in the context\n",
    "    search_matches = search_dc_requests(query, top_k=3)\n",
    "    potential_request_types = [match['request_type'].lower() for match in search_matches]\n",
    "    if request_type.lower() not in potential_request_types:\n",
    "        evaluation_result.update({\"rt_complete\": False, \"resolution_estimate_complete\": False, \"complete\": False})\n",
    "        index_of_req = -1\n",
    "    else:\n",
    "        index_of_req = potential_request_types.index(request_type.lower())\n",
    "\n",
    "    evaluation_result[\"potential_request_types\"] = potential_request_types\n",
    "\n",
    "    # 3. Check if the estimated resolution time is stated in the answer\n",
    "    known_resolution_estimates = []\n",
    "    for elem in search_matches:\n",
    "        known_resolution_estimates.append(elem[\"resolution_estimate\"])\n",
    "    \n",
    "    if index_of_req > -1:\n",
    "        ai_resolution_estimate = known_resolution_estimates[index_of_req]\n",
    "        ai_resolution_days = ai_resolution_estimate.split(' ')[0]\n",
    "        if (ai_resolution_days not in ai_response or \n",
    "        (ai_resolution_days == '1' and not any(x in ai_response for x in ['1', 'one']))):\n",
    "            evaluation_result.update({\"resolution_estimate_complete\": False, \"complete\": False})\n",
    "    \n",
    "\n",
    "    # 4. Re-Prompt to Check Correctness\n",
    "    context_lines = []\n",
    "    for res in search_matches:\n",
    "        context_lines.append(\n",
    "            f\"Request Type: {res['request_type']}\\n\"\n",
    "            f\"Department: {res['department']}\\n\"\n",
    "            f\"Resolution Estimate: {res['resolution_estimate']}\\n\"\n",
    "            f\"Description: {res['description']}\\n\"\n",
    "            f\"Distance: {res['distance']}\\n\"\n",
    "            \"----\"\n",
    "        )\n",
    "    context_str = \"\\n\".join(context_lines)\n",
    "    correctness_check = reprompt_for_correctness(query, ai_response, context_str)\n",
    "    \n",
    "    evaluation_result[\"is_correct\"] = correctness_check.get(\"is_correct\")\n",
    "    evaluation_result[\"revised_answer\"] = correctness_check.get(\"revised_answer\")\n",
    "\n",
    "    return evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Do I need a permit to build a fence around my yard?\n",
      "AI Answer: In Washington D.C., you generally do not need a permit to build a fence around your yard if it meets certain requirements. However, it's advisable to check with the Department of Consumer and Regulatory Affairs (DCRA) to confirm if your specific project falls within the exemption criteria.\n",
      "\n",
      "Used request type: Not sure\n",
      "\n",
      "EVALUATION OF AI RESPONSE:\n",
      "\n",
      "READABILITY:\n",
      "Flesch reading ease score: 49.82\n",
      "Gunning fog index: 12.89\n",
      "\n",
      "Potential request types: ['dob - illegal construction', 'residential parking permit violation', 'resident parking permit']\n",
      "\n",
      "COMPLETENESS:\n",
      "Flagged Response: The AI response is not complete.\n",
      "The response does not include the estimated time for resolution of the request.\n",
      "The response does not state which request type the request belongs to. The question might not be in the context.\n",
      "\n",
      "CORRECTNESS:\n",
      "AI response seems to be correct.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Do I need a permit to build a fence around my yard?\"\n",
    "# user_query = \"I started seeing lots of dead rats on my street. What can I do about this?\"\n",
    "# user_query = \"There is always trash left in front of my building. What can I do about this?\"\n",
    "\n",
    "ai_answer = generate_response(user_query)\n",
    "try:\n",
    "    request_type = ai_answer.split(\"Used request type: \")[1]\n",
    "except:\n",
    "    request_type = 'Not found'\n",
    "\n",
    "eval_result = evaluate_response_with_rules(user_query, ai_answer, context_df, request_type)\n",
    "\n",
    "print(\"User Query:\", user_query)\n",
    "print(\"AI Answer:\", ai_answer)\n",
    "print(\"\\nEVALUATION OF AI RESPONSE:\\n\")\n",
    "\n",
    "print(\"READABILITY:\")\n",
    "print(f\"Flesch reading ease score: {eval_result['flesch_reading_ease']}\")\n",
    "print(f\"Gunning fog index: {eval_result['gunning_fog']}\")\n",
    "\n",
    "print(f\"\\nPotential request types: {eval_result['potential_request_types']}\")\n",
    "\n",
    "print(\"\\nCOMPLETENESS:\")\n",
    "if not eval_result[\"complete\"]:\n",
    "    print(\"Flagged Response: The AI response is not complete.\")\n",
    "    if not eval_result['resolution_estimate_complete']:\n",
    "        print(\"The response does not include the estimated time for resolution of the request.\")\n",
    "    if not eval_result['rt_complete']:\n",
    "        print(\"The response does not state which request type the request belongs to. The question might not be in the context.\")\n",
    "else:\n",
    "    print('AI response is complete.')\n",
    "\n",
    "print(\"\\nCORRECTNESS:\")\n",
    "if not eval_result['is_correct']:\n",
    "    print(\"The answer might not be correct. Here's the revised response:\")\n",
    "    print(eval_result['revised_answer'])\n",
    "else:\n",
    "    print('AI response seems to be correct.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
