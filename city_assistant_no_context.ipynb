{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "df = pd.read_csv('./data/resident_request_questions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtain Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    response = openai.embeddings.create(input=[text], model=model)\n",
    "    embedding = response.data[0].embedding\n",
    "    return np.array(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embeddings for question dataset\n",
    "embeddings = []\n",
    "for inquiry in df['question']:\n",
    "    embeddings.append(get_embedding(inquiry))\n",
    "\n",
    "embeddings = np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_f32 = embeddings.astype(np.float32)\n",
    "embedding_dim = embeddings_f32.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "faiss_index.add(embeddings_f32)\n",
    "faiss.write_index(faiss_index, \"./faiss/sample_questions_faiss_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To simply load pre-calculated, run this:\n",
    "faiss_index = faiss.read_index(\"./faiss/sample_questions_faiss_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_inquiries_faiss(query: str, top_k: int = 3):\n",
    "    query_vec = get_embedding(query)\n",
    "    query_vec_f32 = np.array([query_vec]).astype(np.float32)\n",
    "    distances, indices = faiss_index.search(query_vec_f32, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for rank, idx in enumerate(indices[0]):\n",
    "        row = df.iloc[idx]\n",
    "        dist = distances[0][rank]  # L2 distance\n",
    "        results.append({\n",
    "            \"question\": row[\"question\"],\n",
    "            \"request_type\": row[\"request_type\"],\n",
    "            \"distance\": float(dist)\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "def search_similar_inquiries(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Given a query, find the top_k similar inquiries from the dataset using cosine similarity.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "    top_indices = similarities.argsort()[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"question\": df.iloc[idx][\"question\"],\n",
    "            \"request_type\": df.iloc[idx][\"request_type\"],\n",
    "            \"similarity_score\": float(similarities[idx])\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI system that answers city residents' questions.\n",
    "Provide clear, concise, and legally compliant responses.\n",
    "If applicable, refer to the legal or FAQ documents of one of the following departments of Washington D.C.:\n",
    "-Parking Enforcement Management Administration\n",
    "-Urban Forrestry\n",
    "-Trans Sys Mnt-Signs\n",
    "-Driver Vehicle Services\n",
    "-Solid Waste Management Administration\n",
    "-Transportation Operations Administration\n",
    "-SNOW\n",
    "-SIOD\n",
    "-Tru-311 \n",
    "-Toa-Street & Bridge Maintenance\n",
    "-Adjudication Services\n",
    "-DC Interagency Council on Homelessness\n",
    "-Department of Energy and Environment\n",
    "-FEMS-Special Events\n",
    "-HOMYDRPR- How Is My Driving Program\n",
    "-Toa- Trans Sys Mnt\n",
    "-Department Of Health\n",
    "-Department of Transportation\n",
    "-FEMS-Smoke Alarms\n",
    "-Transportation Policy & Planning Administration\n",
    "-Department of Disability Services \n",
    "-Department of Buildings\n",
    "Mention specific clauses or sentences from documents if possible.\n",
    "If you are uncertain about the correct answer, politely say so and prompt for clarification.\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(user_query):\n",
    "    similar_inq = search_similar_inquiries_faiss(user_query, top_k=2)\n",
    "    \n",
    "    context_info = \"\\n\".join(\n",
    "        [f\"- Similar question: {res['question']} (request_type: {res['request_type']})\"\n",
    "         for res in similar_inq]\n",
    "    )\n",
    "    final_prompt = f\"{SYSTEM_PROMPT}\\n\\nContext from similar questions:\\n{context_info}\\n\\nUser question: {user_query}\\nAI answer:\"\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": final_prompt},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(user_query, ai_response):\n",
    "    \"\"\"\n",
    "    A simple heuristic-based approach to check:\n",
    "    1. Is the response at least X characters? (completeness)\n",
    "    2. Does the response contain disclaimers if uncertain? (correctness check)\n",
    "    3. Is the language appropriate for a layperson? (basic readability check)\n",
    "    \n",
    "    More sophisticated evaluation would parse the AI response,\n",
    "    compare to a known FAQ or use another LLM for meta-evaluation.\n",
    "    \"\"\"\n",
    "    evaluation_result = {\n",
    "        \"complete\": False,\n",
    "        \"flags\": [],\n",
    "        \"message\": \"\"\n",
    "    }\n",
    "    \n",
    "    # 1. Check length (very naive completeness check)\n",
    "    if len(ai_response) > 50:\n",
    "        evaluation_result[\"complete\"] = True\n",
    "    \n",
    "    # 2. Check for uncertain disclaimers\n",
    "    if \"uncertain\" in ai_response.lower() or \"clarification\" in ai_response.lower():\n",
    "        # Possibly a correct approach if the question is truly ambiguous\n",
    "        evaluation_result[\"flags\"].append(\"uncertainty\")\n",
    "    \n",
    "    # 3. Basic readability (e.g., count jargon words, or a simpler method)\n",
    "    # This is a placeholder. In real usage, you might do a Flesch-Kincaid score, etc.\n",
    "    if any(word in ai_response.lower() for word in [\"therefore\", \"henceforth\"]):\n",
    "        evaluation_result[\"flags\"].append(\"potentially_complex_language\")\n",
    "    \n",
    "    # Summarize\n",
    "    if evaluation_result[\"complete\"]:\n",
    "        evaluation_result[\"message\"] = \"Response meets basic length requirement.\"\n",
    "    else:\n",
    "        evaluation_result[\"message\"] = \"Response might be incomplete. Consider rechecking.\"\n",
    "    \n",
    "    return evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: There is always trash left in front of my building. What can I do about this?\n",
      "AI Answer: If there is always trash left in front of your building, you can report this issue to the Solid Waste Management Administration in Washington D.C. They can provide guidance on proper waste disposal and address any sanitation concerns in your area. You can reach out to them for assistance in resolving the trash accumulation in front of your building.\n",
      "Evaluation: {'complete': True, 'flags': [], 'message': 'Response meets basic length requirement.'}\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Do I need a permit to build a fence around my yard?\"\n",
    "user_query = \"There is always trash left in front of my building. What can I do about this?\"\n",
    "\n",
    "ai_answer = generate_response(user_query)\n",
    "eval_result = evaluate_response(user_query, ai_answer)\n",
    "\n",
    "print(\"User Query:\", user_query)\n",
    "print(\"AI Answer:\", ai_answer)\n",
    "print(\"Evaluation:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "city_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
